{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6945939,"sourceType":"datasetVersion","datasetId":3989137}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Cell 1: Check Data Directories\nThis step remains the same.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/1000-videos-split/1000_videos/train/fake | head -n 5\n!ls /kaggle/input/1000-videos-split/1000_videos/train/real | head -n 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.229631Z","iopub.execute_input":"2025-09-21T18:32:53.229963Z","iopub.status.idle":"2025-09-21T18:32:53.522635Z","shell.execute_reply.started":"2025-09-21T18:32:53.229916Z","shell.execute_reply":"2025-09-21T18:32:53.521833Z"}},"outputs":[{"name":"stdout","text":"128_896_10.png\n128_896_11.png\n128_896_12.png\n128_896_13.png\n128_896_14.png\nls: write error: Broken pipe\n129_10.png\n129_2.png\n129_3.png\n129_4.png\n129_5.png\nls: write error: Broken pipe\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Hardware Detection\n\nThis cell runs a check to identify the available hardware and selects the appropriate TensorFlow distribution strategy.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\n\ndef get_distribution_strategy():\n    \"\"\"\n    Detects available hardware (TPU, multi-GPU, single-GPU, CPU) and returns\n    the appropriate TensorFlow distribution strategy.\n    \"\"\"\n    try:\n        # Attempt to detect and initialize a TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(\"✅ Running on TPU\")\n    except (ValueError, tf.errors.NotFoundError):\n        # If no TPU is found, check for GPUs\n        gpus = tf.config.list_physical_devices('GPU')\n        if len(gpus) > 1:\n            # If multiple GPUs are available, use MirroredStrategy\n            strategy = tf.distribute.MirroredStrategy()\n            print(f\"✅ Running on {len(gpus)} GPUs\")\n        elif len(gpus) == 1:\n            # If a single GPU is available, use the default strategy\n            strategy = tf.distribute.get_strategy()\n            print(\"✅ Running on a single GPU\")\n        else:\n            # If no GPUs are found, run on CPU\n            strategy = tf.distribute.get_strategy()\n            print(\"✅ Running on CPU\")\n            \n    print(f\"Number of accelerator replicas: {strategy.num_replicas_in_sync}\")\n    return strategy\n\n# Run the detection function to see what hardware is available\nstrategy = get_distribution_strategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.524286Z","iopub.execute_input":"2025-09-21T18:32:53.524530Z","iopub.status.idle":"2025-09-21T18:32:53.531892Z","shell.execute_reply.started":"2025-09-21T18:32:53.524507Z","shell.execute_reply":"2025-09-21T18:32:53.531314Z"}},"outputs":[{"name":"stdout","text":"✅ Running on a single GPU\nNumber of accelerator replicas: 1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile model.py\nimport tensorflow as tf\n# tf.config.run_functions_eagerly(True)\n\ndef backbone():\n    '''\n    RETURNS THE BACKBONE FEATURE ENCODER NETWORK\n    XCEPTION USED IN THIS CASE\n    '''\n    mod  = tf.keras.applications.Xception(weights='imagenet')\n    mod = tf.keras.Model(mod.input, mod.layers[-13].output)\n    return mod\n    \nclass ModifiedBranch(tf.keras.layers.Layer):\n    def __init__(self, a_vec_size, **kwargs):\n        # --- FIX 1: Accept **kwargs and pass them to the parent class ---\n        super(ModifiedBranch, self).__init__(**kwargs)\n        self.a_vec_size = a_vec_size\n        self.dense_layer = tf.keras.layers.Dense(self.a_vec_size, activation='tanh')\n    # --- ADD THIS METHOD TO OVER COME WARRNING---\n    def build(self, input_shape):\n        super(ModifiedBranch, self).build(input_shape)\n\n    def call(self, input):\n        af = tf.keras.backend.mean(input, axis=2) \n        hs = self.dense_layer(af)\n        return hs\n\n    # --- FIX 2: Add get_config to save your custom arguments ---\n    def get_config(self):\n        config = super(ModifiedBranch, self).get_config()\n        config.update({\n            \"a_vec_size\": self.a_vec_size,\n        })\n        return config\n\n\nclass MainBranch(tf.keras.layers.Layer):\n    def __init__(self, a_vec_size, dim, **kwargs):\n        # --- FIX 1: Accept **kwargs and pass them to the parent class ---\n        super(MainBranch, self).__init__(**kwargs)\n        self.a_vec_size = a_vec_size\n        self.dim = dim\n        self.dropout_layer = tf.keras.layers.Dropout(0.5)\n\n    def call(self, input):\n        e = tf.transpose(input, perm=[0, 2, 1])\n        e = tf.keras.layers.Reshape((-1, self.a_vec_size))(e)\n        e = tf.keras.activations.relu(e)\n        e = self.dropout_layer(e)\n        e = tf.keras.layers.Reshape((self.dim**2, self.a_vec_size))(e)\n        e = tf.transpose(e, perm=[0, 2, 1])\n        return e\n\n    # --- FIX 2: Add get_config to save your custom arguments ---\n    def get_config(self):\n        config = super(MainBranch, self).get_config()\n        config.update({\n            \"a_vec_size\": self.a_vec_size,\n            \"dim\": self.dim,\n        })\n        return config\n\n\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, dim, a_vec_size, **kwargs):\n        # --- FIX 1: Accept **kwargs and pass them to the parent class ---\n        super(Attention, self).__init__(**kwargs)\n        self.dim = dim\n        self.a_vec_size = a_vec_size\n        self.dense_eh = tf.keras.layers.Dense(self.dim**2)\n        self.dense_final = tf.keras.layers.Dense(1, use_bias=False)\n        self.add_layer = tf.keras.layers.Add()\n        self.dropout_layer = tf.keras.layers.Dropout(0.5)\n\n    def call(self, input):\n        eh = self.dense_eh(input[0])\n        eh = tf.keras.layers.Reshape((1, self.dim**2))(eh)\n        eh = self.add_layer([input[1], eh])\n        eh = tf.keras.activations.relu(eh)\n        eh = self.dropout_layer(eh)\n        eh = tf.transpose(eh, perm=[0, 2, 1])\n        eh = tf.keras.layers.Reshape((-1, self.a_vec_size))(eh)\n        eh = self.dense_final(eh)\n        eh = tf.keras.layers.Reshape((-1, self.dim**2))(eh)\n        eh = tf.keras.activations.relu(eh)\n        return eh\n        \n    # --- FIX 2: Add get_config to save your custom arguments ---\n    def get_config(self):\n        config = super(Attention, self).get_config()\n        config.update({\n            \"dim\": self.dim,\n            \"a_vec_size\": self.a_vec_size,\n        })\n        return config\n\n\ndef model(a_vec_size, dim):\n    '''\n    THIS FUNCTION CALLS THE ENTIRE MODEL\n    INPUT : a_vec_size, dim\n    a_vec_size = number of hidden nodes used in attention mechanism\n    dim = output feature map dimension of backbone network\n    OUTPUT : mod\n    mod = built model\n    '''\n    back = backbone() #CALLING THE BACKBONE NETWORK\n    backbone_feature = back.output  \n    out = tf.keras.layers.Conv2D(filters = a_vec_size, kernel_size = (1,1), strides=(1,1), padding = 'valid', use_bias=True)(backbone_feature) #APPLYING 1X1 CONVOLUTION\n    out = tf.keras.layers.BatchNormalization(axis=-1)(out)\n    out = tf.keras.activations.relu(out)\n    out = tf.keras.layers.Dropout(0.8)(out)\n    out = tf.keras.layers.Reshape((a_vec_size, dim**2))(out) #RESHAPED TO DIMENSION (1024, 381)\n    #THIS OUTPUT IS PASSED THROUGH TWO BRANCHES\n    modified = ModifiedBranch(a_vec_size)(out) #FIRST BRANCH WHICH TRANSFORMS THE OUTPUT FEATURE MAPS GENERATED BY BACKBONE\n    main = MainBranch(a_vec_size, dim)(out) #SECOND BRANCH \n    att = Attention(dim, a_vec_size)([modified, main]) #USING ATTENTION BETWEEN THE TWO BRANCHES\n    fin = tf.keras.layers.Dense(2, activation='softmax')(att) #CLASSIFICATION LAYER\n    fin = tf.keras.layers.Flatten()(fin)\n    mod = tf.keras.Model(inputs=back.input, outputs=fin) #MODEL BUILT\n    return mod","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.532683Z","iopub.execute_input":"2025-09-21T18:32:53.532924Z","iopub.status.idle":"2025-09-21T18:32:53.549337Z","shell.execute_reply.started":"2025-09-21T18:32:53.532902Z","shell.execute_reply":"2025-09-21T18:32:53.548683Z"}},"outputs":[{"name":"stdout","text":"Overwriting model.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile utils.py\nimport os\nimport cv2\nimport glob\nimport numpy as np\n\ndef get_input(path):\n    im = cv2.imread(path)\n    return(im)\n\ndef get_files(path, ext):\n    files = []\n    label_files= []\n    for x in os.walk(path):\n        for y in glob.glob(os.path.join(x[0], '*.{}'.format(ext))):\n            files.append(y)\n    label_files = ['fake', 'real']\n    return files, label_files\n\n\n# def get_output(path, label_file):\n#     img_id = path.split('/')[-1].split('_')[0]\n#     laba = []\n#     for label in label_file:\n#       if label == img_id:\n#         laba.append(1)\n#       else:\n#         laba.append(0)\n#     return laba\n# Corrected logic for utils.py\ndef get_output(path, label_file):\n    # The true label is the name of the parent folder (e.g., 'fake' or 'real')\n    true_label = path.split('/')[-2]\n\n    # Create a one-hot encoded vector\n    # e.g., if true_label is 'fake', it returns [1, 0]\n    # e.g., if true_label is 'real', it returns [0, 1]\n    if true_label == 'fake':\n        return [1, 0]\n    else: # Assumes the only other option is 'real'\n        return [0, 1]\n\n\n\n\ndef image_generator(files, label_files, batch_size, resize=None):\n    while True:\n          batch_paths  = np.random.choice(a  = files, \n                                          size = batch_size)\n          batch_x = []\n          batch_y = [] \n          \n          for input_path in batch_paths:\n              input = get_input(input_path)\n              output = get_output(input_path, label_files)\n              if resize is not None:\n                input = cv2.resize(input, resize)\n              batch_x.append(input)\n              batch_y.append(output)\n\n          batch_x = np.array(batch_x)\n          batch_y = np.array(batch_y)\n          yield batch_x, batch_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.550731Z","iopub.execute_input":"2025-09-21T18:32:53.550918Z","iopub.status.idle":"2025-09-21T18:32:53.567133Z","shell.execute_reply.started":"2025-09-21T18:32:53.550904Z","shell.execute_reply":"2025-09-21T18:32:53.566425Z"}},"outputs":[{"name":"stdout","text":"Overwriting utils.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile train.py\nimport os\nimport utils\nimport model\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # Add EarlyStopping here\n# tf.config.run_functions_eagerly(True)\n\nclass train():\n    '''\n    Class used to train the model used in deep fake image detection\n    INPUT : train_path, val_path, epochs, batch_size, steps\n    train_path = absolute path of the training image set\n    val_path = absolute path of the calidation image set\n    epochs = Number of epochs to be used for training the model\n    batch_size = Batch size to be used per step\n    steps = Number of steps to be used per epoch\n    '''\n    def __init__(self, train_path, val_path):\n        self.train_path = train_path\n        self.val_path = val_path\n        here = os.path.dirname(os.path.abspath(__file__))\n        self.path = os.path.join(here, \"models\")\n    \n    def get_files(self):\n        self.train_files, self.label_files = utils.get_files(self.train_path, 'png')\n        self.val_files, self.label_files = utils.get_files(self.val_path, 'png')\n\n    # @tf.function\n    def train(self, model, path, epochs, batch_size, steps, dim):\n        \"\"\"\n        Compiles and trains the deep learning model.\n        \"\"\"\n        model.compile('Adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n        \n        checkpoint_filepath = os.path.join(path, \"best_model.keras\")\n        \n        # This callback saves the best model found so far\n        model_checkpoint_callback = ModelCheckpoint(\n            filepath=checkpoint_filepath,\n            save_best_only=True, \n            save_weights_only=False, \n            monitor='val_accuracy', \n            mode='max'\n        )\n\n        # --- STEP 2: Create the EarlyStopping callback ---\n        # This callback will stop training if 'val_loss' does not improve for 5 epochs\n        early_stopping_callback = EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        )\n\n        # Start the training process\n        model.fit(\n            utils.image_generator(self.train_files, self.label_files, batch_size, dim), \n            epochs=epochs, \n            steps_per_epoch=steps,\n            validation_data=utils.image_generator(self.val_files, self.label_files, batch_size, dim),\n            validation_steps=150,\n            # Add the new callback to the list\n            callbacks=[model_checkpoint_callback, early_stopping_callback]\n        )\n\n    \n    def run(self, epochs, batch_size, steps, dim=(299, 299)):\n        '''\n        DRIVER FUNCTION\n        '''\n        self.get_files()\n        print(\"************TRAINING SOFT ATTENTION BASED DEEP FAKE DETECTION MODEL************\")\n        mod = model.model(1024, 19)\n        self.train(mod, self.path, epochs, batch_size, steps, dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.568220Z","iopub.execute_input":"2025-09-21T18:32:53.568783Z","iopub.status.idle":"2025-09-21T18:32:53.584034Z","shell.execute_reply.started":"2025-09-21T18:32:53.568765Z","shell.execute_reply":"2025-09-21T18:32:53.583465Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%%writefile main.py\nimport argparse\nfrom train import train\n\ndef main():\n    parser = argparse.ArgumentParser(description='Visual Attention based Deepfake Video Forgery Detection')\n\n    parser.add_argument('--train', type=str, nargs = '+', help = 'What is the path of the training image data?')\n    parser.add_argument('--val', type=str, nargs = '+', help = 'What is the path of the Validation image data?')\n    parser.add_argument('--epochs', type=int, default=50, help = 'What is the training epoch for model?')\n    parser.add_argument('--batch', type=int, default=32, help = 'What is the training batch size?')\n    parser.add_argument('--steps', type=int, default=40, help = 'What is the training steps per epoch?')\n\n    args = parser.parse_args()\n    args.train = ' '.join(args.train)\n    args.val = ' '.join(args.val)\n\n    print(\"Configuration\")\n    print(\"----------------------------------------------------------------------\")\n    print(\"Training Path : {}\".format(args.train))\n    print(\"Validation Path : {}\".format(args.val))\n    print(\"Epochs while training the model : {}\".format(args.epochs))\n    print(\"Batch Size : {}\".format(args.batch))\n    print(\"Steps per epochs : {}\".format(args.steps))\n    print(\"----------------------------------------------------------------------\")\n\n    train(args.train, args.val).run(args.epochs, args.batch, args.steps)\n\nif __name__=='__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.584638Z","iopub.execute_input":"2025-09-21T18:32:53.584871Z","iopub.status.idle":"2025-09-21T18:32:53.601344Z","shell.execute_reply.started":"2025-09-21T18:32:53.584851Z","shell.execute_reply":"2025-09-21T18:32:53.600645Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Example Usage: total file no /batch = steps\n\n!python main.py \\\n    --train \"/kaggle/input/1000-videos-split/1000_videos/train\" \\\n    --val \"/kaggle/input/1000-videos-split/1000_videos/validation\" \\\n    --epochs 50 \\\n    --batch 16 \\\n    --steps 727","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:32:53.602250Z","iopub.execute_input":"2025-09-21T18:32:53.602514Z"}},"outputs":[{"name":"stdout","text":"2025-09-21 18:32:54.126950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758479574.147634   29230 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758479574.153695   29230 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nConfiguration\n----------------------------------------------------------------------\nTraining Path : /kaggle/input/1000-videos-split/1000_videos/train\nValidation Path : /kaggle/input/1000-videos-split/1000_videos/validation\nEpochs while training the model : 50\nBatch Size : 16\nSteps per epochs : 727\n----------------------------------------------------------------------\n************TRAINING SOFT ATTENTION BASED DEEP FAKE DETECTION MODEL************\nI0000 00:00:1758479583.134312   29230 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nEpoch 1/50\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1758479614.756787   29251 service.cc:148] XLA service 0x7c5de0001930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1758479614.756829   29251 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1758479617.635242   29251 cuda_dnn.cc:529] Loaded cuDNN version 90300\nE0000 00:00:1758479627.785967   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1758479627.995454   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1758479629.055519   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1758479629.287344   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1758479630.201839   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1758479630.391732   29251 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1758479639.771286   29251 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 245ms/step - accuracy: 0.6921 - loss: 0.7186 - val_accuracy: 0.8308 - val_loss: 0.5615\nEpoch 2/50\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 240ms/step - accuracy: 0.9344 - loss: 0.1860 - val_accuracy: 0.8725 - val_loss: 0.4458\nEpoch 3/50\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 240ms/step - accuracy: 0.9520 - loss: 0.1315 - val_accuracy: 0.8737 - val_loss: 0.5292\nEpoch 4/50\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 238ms/step - accuracy: 0.9588 - loss: 0.1199 - val_accuracy: 0.8554 - val_loss: 0.4107\nEpoch 5/50\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 240ms/step - accuracy: 0.9637 - loss: 0.1018 - val_accuracy: 0.8871 - val_loss: 0.3602\nEpoch 6/50\n\u001b[1m153/727\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 226ms/step - accuracy: 0.9759 - loss: 0.0727","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!zip -r my_model.zip models/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile predict.py\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport argparse\nimport os\n\n# --- Import your custom layers and preprocessing function ---\nfrom model import ModifiedBranch, MainBranch, Attention\nfrom tensorflow.keras.applications.xception import preprocess_input\n\n# Define the labels list globally\nLABELS = ['fake', 'real']\n\ndef load_and_prep_image(image_path, target_size=(299, 299)):\n    \"\"\"\n    Loads, resizes, and preprocesses a single image.\n    Returns None if the image cannot be read.\n    \"\"\"\n    try:\n        img = cv2.imread(image_path)\n        if img is None:\n            print(f\"Warning: Could not read image {image_path}. Skipping.\")\n            return None\n            \n        img = cv2.resize(img, target_size)\n        img_preprocessed = preprocess_input(img)\n        return np.expand_dims(img_preprocessed, axis=0)\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return None\n\ndef predict_single_image(model, image_path):\n    \"\"\"\n    Loads a single image, predicts it, and prints the result.\n    \"\"\"\n    image_batch = load_and_prep_image(image_path)\n    if image_batch is None: return\n\n    print(\"Predicting...\")\n    prediction = model.predict(image_batch)\n    \n    predicted_index = np.argmax(prediction[0])\n    predicted_label = LABELS[predicted_index]\n    confidence = prediction[0][predicted_index] * 100\n\n    print(\"\\n--- Prediction Result ---\")\n    print(f\"       File: {os.path.basename(image_path)}\")\n    print(f\"Prediction is: {predicted_label.upper()}\")\n    print(f\"  Confidence: {confidence:.2f}%\")\n    print(\"-------------------------\")\n\ndef evaluate_folder(model, folder_path):\n    \"\"\"\n    Recursively evaluates all images in a given folder.\n    \"\"\"\n    print(f\"Scanning folder: {folder_path}\\nThis may take a while...\")\n    total_files = 0\n    correct_predictions = 0\n    \n    for root, dirs, files in os.walk(folder_path):\n        for filename in files:\n            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')): continue\n            true_label = os.path.basename(root).lower()\n            if true_label not in LABELS: continue\n\n            image_path = os.path.join(root, filename)\n            image_batch = load_and_prep_image(image_path)\n            if image_batch is None: continue\n            \n            total_files += 1\n            prediction = model.predict(image_batch, verbose=0)\n            predicted_index = np.argmax(prediction[0])\n            predicted_label = LABELS[predicted_index]\n\n            # --- MODIFICATION IS HERE ---\n            confidence = prediction[0][predicted_index] * 100\n\n            if predicted_label == true_label:\n                correct_predictions += 1\n                result = \"CORRECT\"\n            else:\n                result = \"WRONG\"\n            \n            # --- AND HERE ---\n            print(f\"  > File: {filename} | True: {true_label} | Predicted: {predicted_label} ({confidence:.2f}%)  [{result}]\")\n\n    if total_files > 0:\n        accuracy = (correct_predictions / total_files) * 100\n        print(\"\\n--- Evaluation Summary ---\")\n        print(f\"Total Images: {total_files}\")\n        print(f\"Correct Predictions: {correct_predictions}\")\n        print(f\"OVERALL ACCURACY: {accuracy:.2f}%\")\n        print(\"--------------------------\")\n    else:\n        print(\"\\nNo valid image files found in 'fake' or 'real' subdirectories.\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Predict if an image is real or fake.')\n    parser.add_argument('--input_path', type=str, required=True, help='Path to an image file OR a folder.')\n    parser.add_argument('--model_path', type=str, default='models/best_model.keras', help='Path to the saved model file.')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.model_path):\n        print(f\"Error: Model file not found at {args.model_path}\")\n        return\n\n    custom_objects = {\"ModifiedBranch\": ModifiedBranch, \"MainBranch\": MainBranch, \"Attention\": Attention}\n    print(\"Loading model...\")\n    model = tf.keras.models.load_model(args.model_path, custom_objects=custom_objects)\n    print(\"Model loaded.\")\n\n    if os.path.isfile(args.input_path):\n        predict_single_image(model, args.input_path)\n    elif os.path.isdir(args.input_path):\n        evaluate_folder(model, args.input_path)\n    else:\n        print(f\"Error: Input path is not a valid file or directory: {args.input_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of how to run prediction on a single image from your validation set.\n# You will need to find a valid path to an image.\n\n# !python predict.py --image \"/kaggle/input/1000-videos-split/1000_videos/validation/fake/000_003_0.png\"\n!python predict.py --input_path \"/kaggle/input/1000-videos-split/1000_videos/test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}